Week 9:
Learning
COSC350/550
Artificial Intelligence
               Agenda



1   Learning Agents
2   Supervised Learning

3   Unsupervised Learning
4   Reinforcement Learning
Learning Agents
How can we define learning?
           An agent is learning if it improves over time
           by making new observations

           Examples:
            A baby listening to their parents with time will develop the ability to
            speak
Learning    A cat will learn to associate the sound of croquettes in a box to the

Agents      food being served
            A student practicing with the unit's workshops will learn how to
            solve the unit's assignments


           The agent senses data, it uses it to learn a
           model of the world, and it uses the learned
           model to make appropriate decisions
 Isn't what happened already
   with search algorithms?
 In a way, yes... The agent uses a model of the world and it updates it with new information.

Based on the new information acquired, better decisions are made by using search algorithms
                              with the given updated model
   ... and isn't it what happened with
    logical and probabilistic agents?
In a way, yes... the agent uses a Knowledge Base representing knowledge about the world and it
                                 updates it with new information.

  Based on the new information acquired, new inferences are made by using the updated KB
So... what's new?!
                             Machine Learning

When the agent is a computer that is             Even if we get new data informing the
learning, we are speaking about machine          agent's future decisions, those decisions
learning (ML)                                    will be based on a strictly unchanged set of
                                                 instructions / rules
Machine learning is a subfield of AI and we      If the agent's program does not handle a specific scenario and that
                                                 scenario happens, the agent will not be able to handle it correctly
could spend an entire Unit on it (and in fact,
                                                 Vice-versa with ML the agent may be able to learn from it and
there is a ML Unit at UNE)                       address this new scenario in future by adapting how the agent will
                                                 respond to it
The key difference between ML and
techniques previously covered during the         With previous techniques, the agent does
unit is that, previously, even when the agent    not possess the ability to generalise beyond
was adapting the behaviour due to new            the specific problem is designed for
knowledge, we created code and rules
specifically handling every considered           With ML techniques, the agent has the
scenario                                         ability to learn from data and generalise
                                                 patterns; it can apply the learned
                                                 knowledge to new, unseen data
            Two ways to build the model about the world

Offline learning:




Online learning:
How can the agent use the model?

 To learn a direct mapping between state and
 actions

 To learn relevant properties of the environment
 from percepts

 To learn how the environment evolves through
 time and make predictions about the results of
 actions

 To learn the utility of states and desirability of
 actions
           Supervised Learning



           X is the input data
           Y is the expected output data the model must learn to predict from X

           Unsupervised Learning
Forms of
Learning
           We only have the input data X. We do not have the expected output Y.
           The model learns patterns in X to predict Y, without having access to Y

           Reinforcement Learning



           We have access to (sporadic) reward values associated to specific states
Supervised Learning
            To learn we need data (examples)

            In the case of supervised learning, we have the set of inputs X and their
            corresponding outputs Y. We can structure our dataset O as a set of pairs:




            Each input-output pair (sample) was generated by an unknown function y =
            g(x)

Learning    The learning objective is to find a function h approximating the unknown
            function g
objective

            The difference between h(x) and g(x) is called error
            Therefore, the learning objective is that of finding h minimising the error
            between g(x) and h(x)

            The error can be computed in different ways depending on the nature of the
            problem (see later with loss functions)
              The function h we need to find is called hypothesis and it
              is drawn from a hypothesis space H of possible functions:



              For example, if we are modelling our function as a 2nd degree polynomial,
Definitions   then H will be the set of all possible 2nd degree polynomial functions and h
              will be a specific 2nd degree polynomial function approximating the unknown
              objective function g

              We can also say that h is a model of the data drawn from
              the model class H

              yi is the ground truth (true value) for the input xi
               If we use all the known samples O1:N to train our model, we may risk
               that the model is simply memorising the mapping between input and
               output rather than learning and generalising the mapping

               For example, we can give a student the following samples: (1, 2), (2,
               3), (6, 7), (9, 10)
               If the student memorises the mapping, he will be excellent to
               provide the prediction y given an input from the set {1, 2, 6, 9}.
Memorising     However, if given a new observation 3, the student may not be able
               to provide a correct answer
Vs. Learning   If the student learned the underlying laws governing the
               mapping (i.e. the underlying function), he may be able to understand
               that y = x + 1, therefore, given the new observation 3, he may be
               able to provide the correct answer 4

               Overfitting: the model's predictions corresponds too closely or
               exactly to a particular set of data. Hence, the model may fail to
               make correct predictions for observations outside that particular set
               of data. The opposite problem leads to underfitting.
                               Example




                                                                                  Image Source: Google




The samples may include some noise. We do not want to learn the noise!
Ockham's razor: prefer the simplest hypothesis h that is consistent with the given examples
               We can mitigate the risk of overfitting by dividing the dataset of
               observations into two subsets, a training and a test set:




               We use the training set to learn the objective function


Training and
Test sets
               The function L is called loss function and one example of loss function is
               the absolute-value loss: L(y, h(x)) = | y - h(x) | we have seen before

               Then, we use the test set to measure the performance of the model and
               give to it an unbiased evaluation




               The performance measures may inform us on its ability to generalise
       Sometimes we also use a third set, the
       validation set, to fine tune the
       hyperparameters of the model

       There are techniques, such as cross-fold
       validation, that can be used to more
Note   efficiently train and test models

       In this unit we are not going into these
       details, as they are topics more related to the
       field of Machine Learning (for which there is
       a specific unit)
           A decision tree is a tree-like representation of a function
           mapping a vector of features to a single output value, a
           decision




           Each non-leaf node of the tree represents a feature. The
Decision   same feature can appear in many non-leaf nodes.
Tree       Each leaf node of the tree represents a decision. The same
           decision can appear in many leaf nodes.

           Each directed edge represents a possible value for the
           feature of the parent node.

           Each path from the root node to each leaf node represents
           a possible input vector of features.
Example

          Features:
          Patrons: {None, Some, Full}
          WaitEstimate: {0-10, 10-30, 30-60, >60}
          Alternate: {Yes, No}
          Hungry: {Yes, No}
          Reservation: {Yes, No}
          Fri/Sat: {Yes, No}
          Bar: {Yes, No}
          Raining: {Yes, No}

          Decision:
          WaitForTable: {Yes, No}

          DT(<Full, 30-60, No, *, No, *, Yes, *>) =
          Yes
                 A boolean decision tree (i.e. the decision is boolean) is
                 equivalent to the logical statement:




                 Where each Pathi is a conjunction of feature-value tests
                 corresponding to a path the root to a leaf node = True
Expressiveness
of Decision
Trees
                 Boolean decision trees can be used to classify samples:
                 either positive examples of the desired class (True), or
                 negative examples (False)

                 With decision trees we don't only get a decision given a
                 set of features but we also get an explanation for why the
                 model generated that decision
           Objective: given a set of examples, find a tree consistent
           with those examples that is as small as possible

           Divide-and-conquer strategy: always test the most
           important feature first, then recursively solve the smaller
Learning   subproblems defined by the possible results of the tests
Decision   The "most important feature" is a feature that makes the
Trees      most difference to the classification of an example, so that
           we aim for the smaller number of tests, i.e. shallow trees

           The importance of a feature can be computed using the
           notion of information gain, which is defined in terms of
           entropy
          Entropy is a measure of the uncertainty of a random
          variable; the more information, the less entropy
          E.g. A coin always coming up heads has no uncertainty and its entropy is 0

          In general, the entropy of a RV V with k possible values {v1,
          ..., vk} having probability P(vi) can be measured as:



Entropy
          When the RV V is boolean with probability P(V=True) = q,
          we can easily develop the sum for the two possible values
          True (positive example) / False (negative example)
              A feature fi with d distinct values will divide the training set into d subsets Otrain-1, ..., Otrain-d

              Each subset Otrain-k will contain pk positive examples and nk negative examples

              The remaining entropy after going along that branch k can be computed as:




              Therefore, the remaining entropy after testing the feature fi is:

Information
Gain

              And we can compute the information gain for the feature fi as:




              By computing the information gain of all the features, we can choose the one with highest
              gain to split the decision tree
                                                                Example

Scenario: Restaurant, whether to wait for a table or not

Samples: 12, 6 positive (True) and 6 negative (False)

Features for this example: Type, Patrons
Type: French (# True = 1, # False = 1), Italian (# True = 1, # False = 1), Thai (# True = 2, # False = 2), Burger (# True = 2, # False = 2)
Patrons: None (# True = 0, # False = 2), Some (# True = 4, # False = 0), Full (# True = 2, # False = 4)

Information Gains:




Note: B(0.5) = 1, B(0) = 0, B(1) = 0
Decision Tree Learning (Pseudocode)
Unsupervised Learning
            Again, to learn we need data (examples)

            In the case of unsupervised learning, we have the set of inputs X but
            not their corresponding outputs Y:




            We want to learn patterns, structure, or meaningful representations
            in the samples x1, ..., xn
Learning    Given a function J(ri) providing a cost for the representation ri given
objective   the sample xi, the learning objective is to find a function h minimising
            J given the samples x1, ..., xn in X




            This time we don't have an error, but rather a measure of the quality
            of the current representation given the input samples in X
            We can use different cost functions J depending on the problem and
            the desired properties we want to achieve in the new representation
               Clustering Algorithms
               Algorithms like k-means, hierarchical clustering, and Gaussian mixture models aim to
               group similar data points into clusters based on their intrinsic patterns.


               Dimensionality Reduction Techniques
               Principal Component Analysis (PCA), t-distributed Stochastic Neighbour Embedding (t-
               SNE), and Singular Value Decomposition (SVD) are examples of unsupervised methods
               that reduce the dimensionality of the data while preserving important structure.

Classes of     Autoencoders
Unsupervised   These models aim to learn a compressed representation of the input data by encoding it
               into a lower-dimensional space (encoder) and then reconstructing it back to the original
Learning       space (decoder).


Models         Generative Models
               Generative models like Variational Autoencoders (VAEs) and Generative Adversarial
               Networks (GANs) learn to generate new data samples that resemble the original training
               data, which implies that they must learn a meaningful representation of the data.


               In this lecture we will not delve into the details of these
               techniques, bur we will rather give a quick overview
               Units on Machine Learning, Data Science and Statistical Models address these models
               more in-depth
                                                    Clustering
We have a set of samples for which we do not know their labels.
However, we still want to classify them, somehow:




Of course, we cannot use the information Y* during the training of
the model but we may have some test samples with the correct
labels y* to measure the performance of the model during the test
phase
E.g.
X = Millions of unlabelled images from the web
We do not have the resources to label all these samples, but we can do so for a small subset
of them to use as the test set
Sometimes, Y is not available even during the test phase
                 We have a set of samples for which we do not know their
                 labels. Their dimensionality is high and we want to reduce
                 it by also preserving the original structure and information:



Dimensionality
Reduction
Techniques
                 Z is the latent representation of X

                 What does it mean to preserve the original
                 information/structure?
                 E.g. we may want to make sure that classifying X and classifying the new
                 representation Y leads to similar classification performance
               Autoencoders address a similar problem to that of
               dimensionality reduction models, but with an additional
               requirement

               We have a set of samples for which we do not know their
               labels. Their dimensionality is high and we want to reduce it
               by making sure that we can reconstruct the original input
               from the lower dimensionality representation:


Autoencoders


               Again, Z is the latent representation of X (encoding)

               For what can we use autoencoders?
               For example for image compression, information retrieval, or as a more strong
               dimensionality reduction model (not only going in one direction but in both)
             Similar concept to autoencoders but with an addition

             We have a set of samples for which we do not know their
             labels. We want to find a lower dimensionality latent space
             from which we can sample points that we can reconstruct
             as new observations



Generative
Models
             We are looking for a latent representation Z and at the same
             time trying to learn the posteriori of observing a sample x
             given a latent sample z

             The model can easily interpolate latent representations to re-
             construct new unobserved (artificial) data

             For what can we use generative models?
             For example for the generation of artificial data and data reconstruction
Reinforcement Learning
           In Reinforcement Learning (RL), we do not have
           data that we can use to train our model with
           classic supervised or unsupervised models

           Instead, we have sporadic rewards that tells to
           the agent if it is doing good (+) or doing not so
           good (-)
Sporadic   Rewards are numeric values that can suggest how good or bad the
           agent is doing
Rewards
           Objective of a RL agent: given the state the
           agent is in, select the optimal action which will
           maximise the long-term expected reward
           provided by the environment

           A RL agent learns over time through experience
           by interacting with the environment
Learning from Rewards
             We don't need to know perfect {x, y}
             samples, we just need to have an idea of
             how good or bad the agent is doing

Advantages   We don't need large training sets, we learn
             with time, by doing
of RL
             If the reward function is correct, RL is a
             powerful and general way for machines to
             learn, potentially surpassing human
             capabilities
             The rewards may be sparse and delayed
             after several transitions to different states
             Credit assignment problem: what are the causal relationships between
             past decisions and future consequences?


             It may take time to converge to a solution
Challenges
of RL        It is not always easy to design a reward
             function for a specific problem

             The reward function defines what the agent
             would learn. If the reward function is biased,
             the agent will learn these biases.
              There are different types of RL algorithms,
              generally divided between model-based and
              model-free RL

              Model-based RL
              The agent uses a transition model of the environment to help interpret
              the reward signals and to make decisions about how to act. At the
Types of RL   beginning the transition model may be unknown and updated after
              each step by observing the consequences of the agent's actions.
algorithms
              Model-free RL
              In these approaches the agent neither knows nor learns a transition
              model for the environment. Two approaches:
              Action-utility learning: the agent learns a function returning the utility of
              taking an action at a given state and, therefore, selects the action with
              highest expected utility
              Policy search: the agent learns a policy function mapping directly from
              state to action
                       Spoiler alert!
There is quite a bit of Algebra involved when modelling RL models and we will also
                      need knowledge from probabilistic models

The following contents may be overwhelming at first, but if you review them slowly,
             you will see that the underlying concepts are quite simple
           Before looking at the details of RL models, we need to
           start from another model: the Markov Decision Process
           (MDP)

           An MDP is defined by a tuple (S, A, T, R):
            S is the set of states
            A is the set of actions, with A(s) the set of actions when in state s
            T is the model, with T(s, a, s') ~ P(s' | s, a) (transition probability)

Markov      R is the reward function, with R(s, a, s') being the reward obtained after choosing the action a
            when in state s and ending in state s'

Decision   If the environment is deterministic, the model T returns a
Process    single values s' instead of a probability distributions over
           states

           The state and action spaces may be finite or infinite

           Goal: Find a policy function π mapping states S to actions
           A such that an agent following this policy when selecting
           actions would achieve the highest discounted sum of
           expected rewards over a (potentially infinite) horizon
           The policy function determines how the agent chooses
           actions




           Deterministic policy function:

Policy
Function   Given a state s, there is 100% certainty to choose a specific action a

           Stochastic policy function:



           Give a state s, there is a probability distribution over the available actions for
           that state
How do we compute the expected
discounted sum of future rewards?
           Given a state s, assuming I take the best action now and at each future
           step, what long-term reward should I expect? In other words, what is the
           utility (or value) of being in that state?

           The value function aims answering this question:




Value      V(s) is the utility (value) of state s
           R(s, a, s') is the reward obtained by performing action a at state s reaching state s'

Function   E denotes the expected value, i.e. the weighted average of all possible outcomes

           γ (gamma) is the discount factor with value [0, 1] and telling us how much
           we want to factor in the future rewards.
           If gamma is 0, we only look at the immediate reward
           If gamma is 1, we never discount future rewards and always factor them in completely
           For 0 < gamma < 1, we reduce the importance of future rewards over time

           The horizon can be infinite (like in the equation above) or finite

           Typically, we don't have π and we need to learn it so that we can
           maximise the value function V
           We start from the definition of the value function




           Remember that the action a is chosen based on the considered policy π, so
           the value function can be rewritten as:
Optimal
Value
Function
           If we drop the subscript π from V and instead we look for that policy π that
           maximises this expected sum of future rewards, we find the optimal value
           function leading to optimal behaviour (assuming a correct reward model)
       Does the best policy (and therefore an
       optimal value function) always exist?
 Yes! There is always a policy that will lead to maximise the expected sum of discounted future
                                               rewards

However, that does not mean that the optimal policy will necessarily lead to a rational behaviour.
   If the reward function is poorly design and includes biases, the agent will learn to behave
                                           accordingly
           Richard Bellman found out a nice property that allowed him to
           invent Dynamic Programming and opened avenues for RL
           models
           We can decompose the value function into two factors: the
           immediate reward at time 0 and the discounted sum of future
           rewards at time 1 to infinity


Bellman
Equation
           Therefore, we can rewrite the value function by computing
           it recursively




           If we know what the next best state V(s') is, we can simply take an action now to
           get to that next best state and also update the value function for the current state
              If a problem satisfies the Bellman equation, i.e. what we are trying to
              optimise is a recursive function of itself at the next time step, we can
              break up this problem into subproblems

              We can optimise locally long enough and still get a global optimal at
              the end of the process
              Locally in this scenario means to optimise each subproblem V(s') for each s'
              independently and then plugging it in back to V(s) to achieve global optimality

              From a value function, we can always extract the estimated optimal
              policy π:
Dynamic
Programming




              At each state, the agent can use the optimal policy function to select
              the best action maximising the expected sum of future rewards
           Coming back to RL models, when considering model-
           based RL approaches, we have the knowledge of how the
           world evolve, i.e. we know T and R, but we do not have
           any idea yet about the values of states or the policy to
Model-     consider, i.e. we need to learn V and π
Based RL   How do we estimate the value function and the policy
           function?
            Value Iteration algorithm
            Policy Iteration algorithm
            At this point, we have P(s' | s, a) because we know T, we know R but we are
            missing V(s'). However, we can start with a random initialisation of V for all states
            and then iteratively update it until convergence

            Let's define Vi(s) as the value function for state s at the ith iteration of the algorithm.
            V0 is initialised randomly (or 0 for all states).




Value
Iteration
Algorithm


            Epsilon is a small enough threshold that we use to check if the maximum difference
            from the previous value function is small enough and we converged to a solution

            Computational complexity: O(|S|2) for each iteration
            If the problem has a high number of states, e.g. chess, this approach is not feasible
            This algorithm alternates the following two steps, beginning from some initial
            (random) policy π0 and value function (random or all 0):
            1. Policy evaluation step: given a policy πi, calculate Vi as Vπi




            By removing the max operator (because we are using a fixed policy), we are making this computation linear

Policy      We repeat updating Vπi for each state until convergence (like for the value iteration algorithm)
            2. Policy improvement step: we calculate an updated policy πi+1 from the value function we updated during the
Iteration   pevious step


Algorithm


            We alternate these steps until the updated policy computed at step (2) is not
            different from the policy computed during the previous iteration

            This algorithm typically converges in fewer iterations than the value iteration
            algorithm
           The value function and the policy function are intimately related to
           each other because they both try to maximise the expected
           discounted sum of future rewards

           It turns out that we can define a new quality function Q that consider
           the quality of not only a state or an action, but of a state-action pair



Quality
Function
           Given the function Q, we can retrieve V(s) and π(s) from it:




           With Q we can implement model-free RL approaches as we can
           optimise Q with trials and errors without a knowing T and R directly
           Assumption: we don't know how the world evolve (T is unknown)

           We can still learn an optimal policy but we need to do so with trials and errors

           Monte Carlo Learning is an episodic algorithm, i.e. we need to run an entire
           episode (many states and actions in sequence) until getting to a terminal state
           before we can use that information for learning
           So we can use MC learning when the problem has clear terminal states, e.g. games

           Idea: (a) we run an entire episode from an initial state by following a given policy;
Monte      (b) while doing so we sum the discounted rewards; (c) once we reach the terminal
           state, we equally distribute the total reward to all the states that constituted our
Carlo      episode

Learning



           Inefficient way of learning... we need many simulations to learn and we equally
           attribute the reward to all states in an episode
             Idea: instead of averaging the sum of discounted rewards over the states of an
             episode, as soon as we get the reward for a given action a performed at a given
             state s, we compute the error between the current value function estimate and a
             new estimate based on the new gathered reward.

             Starting point:



Temporal
Difference   We don't have T, so we cannot compute V(s) from the above equation. However,
             we can iteratively learn it from:
Learning:
TD(0)


             Note how the TD target estimate is very similar to the value function estimate but
             instead of computing an expected value we simply compute the direct estimate
             given the current reward and the value function obtained so far
           Idea: We use the approach used by the Temporal Learning and
           apply it to the quality function Q

           Remember that



Q-         Therefore:
Learning

           Also, remember that from the Q function we can retrieve the
           policy function
           We can take suboptimal actions, obtain rewards, update Q and then update the
           policy accordingly
                                        Summary

Learning Agents
 A learning agent is an agent improving over time by making new observations
 When the learning agent is a computer, we speak about Machine Learning
 A learning agent can do more than a simple model-based one. It can figure out how to do things it has
 never seen before
 The agent has two approaches for learning a model over time: offline and online learning
 Offline learning requires the observation of all the training data to build a model before using it
 Online learning allows the use of a partially learned model that can then be updated over time with new
 observations
 An agent can use the learned model to predict a mapping between states and actions, to identify
 relevant properties of the environment, to learn how the environment evolves or to learn the utility of
 states and the desirability of actions
 There are three forms of learning: supervised, unsupervised and reinforcement learning
                                  Summary (continue)

Supervised Learning                                       Unsupervised Learning
 In supervised learning the agent has a set of             In unsupervised learning the agent has only a set
 observations X and their corresponding ground-            of observations X without their corresponding
 truth values Y                                            ground-truth values
 The objective of the agent is to find a function          The objective of the agent is to learn patterns,
 h(X) = Y~ such that Y~ approximates Y                     structures or meaningful representations within
                                                           the given observations X
 The objective function h is called hypothesis or
 model                                                     Examples of unsupervised learning models
                                                           include: clustering, dimensionality reduction,
 Learning algorithms divide the observations into          autoencoders and generative models
 a training set, to train the model, and a test set, to
 test the performance of the learned model
 Decision trees are an example of supervised
 learning models using the notion of information
 gain as part of the objective function
                                    Summary (continue)

Reinforcement Learning
 Reinforcement learning (RL) does not have access to examples or labels, but only to sporadic rewards associated to
 specific states and actions
 The objective of RL is to maximise the long-term expected reward provided by the environment
 RL algorithms can be divided into model-based RL, when we know or learn the transition model of the environment, and
 model-free RL, when the transition model is not known nor learned
 RL models comes from Markov Decision Processes (MDPs) that are defined as a tuple (S, A, T, R) of states, actions,
 transitions and rewards
 The objective of a MDP is to find a policy function mapping states to actions that if followed by the agent would maximise
 the expected discounted sum of future rewards
 The value function provides an estimate of the utility of a state in terms of expected future rewards
 From an optimal value function we can extrapolate the optimal policy
 The optimal value function can be recursively estimated using the Bellman principle of optimality (Bellman equation)
 Value function and policy function are interconnected and they can be posed as a quality function offering the utility of a
 state-action pair
 The quality function can be used to estimate the value function and policy function in model-free RL approaches
               Week 10: Biologically Inspired Models of AI

               Recommended activities for Week 9:
                Review the following Chapters:
                  19.1 to 19.3 and 19.4 limited to the loss function

What's next?      16.1 (up to 16.1.2) - 16.2 (up to 16.2.2)
                  23.1 - 23.3 (limited to the topics covered during this lecture)
                Complete the workshop exercises for week 9
                Start working on Assignment 3. Make sure you ask questions if
                something is unclear and seek assistance on the forum, via email or
                with your lab tutor
                If you have questions, please use the forum or send me an email
