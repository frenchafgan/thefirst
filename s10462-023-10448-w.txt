Artificial Intelligence Review (2023) 56:12387–12406
https://doi.org/10.1007/s10462-023-10448-w




Neurosymbolic AI: the 3rd wave

Artur d’Avila Garcez1 · Luís C. Lamb2

Published online: 15 March 2023
© The Author(s), under exclusive licence to Springer Nature B.V. 2023


Abstract
Current advances in Artificial Intelligence (AI) and Machine Learning have achieved
unprecedented impact across research communities and industry. Nevertheless, concerns
around trust, safety, interpretability and accountability of AI were raised by influential
thinkers. Many identified the need for well-founded knowledge representation and rea-
soning to be integrated with deep learning and for sound explainability. Neurosymbolic
computing has been an active area of research for many years seeking to bring together
robust learning in neural networks with reasoning and explainability by offering sym-
bolic representations for neural models. In this paper, we relate recent and early research
in neurosymbolic AI with the objective of identifying the most important ingredients of
neurosymbolic AI systems. We focus on research that integrates in a principled way neu-
ral network-based learning with symbolic knowledge representation and logical reasoning.
Finally, this review identifies promising directions and challenges for the next decade of AI
research from the perspective of neurosymbolic computing, commonsense reasoning and
causal explanation.

Keywords Neurosymbolic AI · Machine learning · Reasoning · Explainable AI · Deep
learning · Trustworthy AI · Cognitive reasoning


1 Introduction

Over the past decade, Artificial Intelligence (in particular, deep learning) has attracted
media attention, have become the focus of increasingly large research endeavors and have
changed businesses. This led to influential debates on the impact of AI in academia and
industry (Marcus 2020). It has been claimed that Deep Learning (DL) caused a para-
digm shift not only in AI, but in several Computer Science fields, including speech rec-
ognition, computer vision, image understanding, natural language processing (NLP), and


* Luís C. Lamb
  luislamb@acm.org
    Artur d’Avila Garcez
    a.garcez@city.ac.uk
1
    Data Science Institute, City, University of London, Northampton Square, London EC1V 0HB, UK
2
    Institute of Informatics, Federal University of Rio Grande do Sul, Av. Bento Gonçalves 9500,
    Porto Alegre, RS 91501‑970, Brazil


                                                                                             13
                                                                                     Vol.:(0123456789)
12388                                                                                A. d. Garcez, L. C. Lamb


machine translation (LeCun et al. 2015). The 2019 and 2020 Montréal AI Debates (Marcus
2020), the AAAI-2021 panel on Neurosymbolic AI, and the AAAI-2020 fireside conver-
sation with Economics Nobel Laureate Daniel Kahneman mediated by Francesca Rossi
and including the participation of the 2018 Turing Award winners and DL pioneers Geof-
frey Hinton, Yoshua Bengio and Yann LeCun, have all pointed to new perspectives on the
future of AI. It has now been argued eloquently that the building of a rich AI system, that
is, a semantically sound, explainable and ultimately trustworthy AI system, will require a
sound reasoning layer in combination with deep learning.
    Parallels have been drawn many times now by AI researchers between Kahneman’s
research on human reasoning and decision making, reflected in his book “Thinking, Fast
and Slow” (Kahneman 2011), and the so-called “AI systems 1 and 2”, which would in
principle be modelled by deep learning and symbolic reasoning, respectively. In this paper,
we seek to place 20 years of research in the area of neurosymbolic AI, known as neu-
ral-symbolic integration, in the context of the recent explosion of interest and excitement
around the combination of deep learning and symbolic reasoning. We revisit early theo-
retical results of fundamental relevance to shaping the latest research, and identify bottle-
necks and the most promising technical directions for the sound representation of learning
and reasoning in neural networks. As well as pointing to the various related and promising
techniques within AI, Machine Learning (ML) and Deep Learning, the paper aims to help
organise some of the terminology commonly used around AI, ML and DL. This seems
important at this exciting time when AI becomes popularized and people from other areas
of Computer Science and from other fields altogether turn to AI: psychology, cognitive
science, economics, medicine, engineering and neuroscience, to name a few. Throughout
the paper, as we make the case for neurosymbolic AI, we will also seek to separate factual
information from our opinion. We shall support our case by pointing to important theoreti-
cal and empirical results of the past 20 years, but also from our observation of the history
of the field: the first wave of AI in the 1980s was symbolic—based on symbolic logic and
logic programming, and later Bayesian networks; the second wave of AI in the 2010s was
neural (or connectionist), based on deep learning. Having lived through both waves and
having seen the contributions and drawbacks of each technology, we argue that the time is
right for the third wave of AI: neurosymbolic AI.
    In Sect. 2, we summarise the current debate around neurons vs. symbols from the per-
spective of the long-standing challenges of variable grounding and commonsense reason-
ing. In Sect. 3, we review some of the prominent forms of neural-symbolic integration. In
Sect. 4, we seek to address neural-symbolic integration from the perspective of distributed
and localist forms of representation. We argue for the importance of a focus on representa-
tion based on the assumption that representation precedes learning and reasoning (Valiant
2003).1 In Sect. 5, we delve deeper into the fundamentals of current neurosymbolic AI
methods and systems. In Sect. 6, we identify promising aspects of neurosymbolic AI to
address exciting challenges for learning, reasoning and explainability. In Sect. 7, we sum-
marise the paper with a list of ingredients for achieving AI and discuss directions for future
research to address the challenges for Artificial Intelligence.




1
  The use of the word representation here is intended to be broader than its use in representation learning
in that the choice of vector space precedes the learning of an embedding.


13
Neurosymbolic AI: the 3rd wave﻿	                                                                 12389


2 Neurons or symbols: the current debate and research question

Deep learning researchers and AI companies have achieved groundbreaking results in
areas such as computer vision, game playing and natural language processing (LeCun
et al. 2015). Despite the impressive results, deep learning has been criticised for brittle-
ness (being susceptible to adversarial attacks), lack of explainability (not having a formally
defined computational semantics or intuitive explanation, leading to questions around
the trustworthiness of AI systems), and lack of parsimony—requiring far too much data,
computational power at training time or unacceptable levels of energy consumption (Mar-
cus 2020). Against this backdrop, leading entrepreneurs and scientists such as Bill Gates
and the late Stephen Hawking have voiced concerns about AI’s accountability, impact on
humanity and the future of the planet.2 Since then, the need for a better understanding of
the underlying principles of AI has become generally accepted. A key question is therefore
that of identifying the necessary and sufficient building blocks of AI and how systems that
evolve automatically based on ML can be developed and analysed in effective ways that
make AI trustworthy and fair.
    Turing award winner Leslie Valiant had pointed out already in 2003 that a key challenge
for Computer Science is the principled combination of reasoning and learning, building a
rich semantics and robust representation language for intelligent cognitive behavior. “The
aim is to identify a way of looking at and manipulating commonsense knowledge that is
consistent with and can support what we consider to be the two most fundamental aspects
of intelligent cognitive behavior: the ability to learn from experience and the ability to rea-
son from what has been learned. We are therefore seeking a semantics of knowledge that
can computationally support the basic phenomena of intelligent behavior” (Valiant 2003).
    The success of deep learning and its now clear limitations have prompted a heated
debate around the value of symbolic AI in contrast with neural computation. A main weak-
ness of DL, as Bengio et al state in a recent article, is that “current machine learning meth-
ods seem weak when they are required to generalize beyond the training distribution, which
is what is often needed in practice” (Bengio et al. 2020). In a series of recent debates on
AI, Gary Marcus has argued for DL to be enhanced with symbols (Marcus 2020). Marcus
argued eloquently for hybrid AI systems and has sought to define what makes an AI system
effectively hybrid. Key to the appreciation of Marcus’s argument is an understanding of the
representational value of the symbolic manipulation of variables in logic.
    Motivated by Valiant’s arguments and Marcus’s insight, the area of neurosymbolic com-
puting has sought to offer a principled way of studying learning and reasoning by establish-
ing provable correspondences between neural models and logical representations (d’Avila
Garcez et al. 2002, 2009, 2015; Lamb et al. 2020), which will be discussed in more detail
later in the paper. We contend that what is needed next is research onto the specific meth-
ods and techniques which seek to address the issues of representation, robustness and
extrapolation. Such methods and techniques are to be drawn from a broad perspective of
neurosymbolic AI, symbolic ML and DL, knowledge representation and reasoning, which
embraces hybrid systems, to address:
    (a) Variable grounding and symbol manipulation Embracing hybrid systems requires
the study of how symbols may emerge and become useful in the context of what DL


2
   Several reports on the future of AI and its consequences for society have been published in the main-
stream media over the past decade. These include the BBC, NY Times, The Observer and others, see e.g.,
https://​www.​bbc.​com/​news/​techn​ology-​37713​629.


                                                                                           13
12390                                                                              A. d. Garcez, L. C. Lamb


researchers have termed disentanglement. Once symbols emerge (which may happen at dif-
ferent levels of abstraction, ideally within a modular network architecture), it may be more
productive from a computational perspective to refer to such symbols and manipulate them
(i.e., compute with them) symbolically. As an example, suppose that it becomes known
that a complex neural network serves the purpose of calculating the sum of two handwrit-
ten digits provided as input images to the network. Once the images are recognised as dig-
its by the network, it is probably the case that one would prefer the calculation to be made
precise and to extrapolate correctly to any sum of digits. This is easily achieved symboli-
cally. Equally, if it is known that a complex neural network has learned the simple function
 f (x) = x then one would probably wish to dispense with the network and use its symbolic
counterpart instead. The challenge, of course, is in the identification of such a precise sym-
bolic description for the complex network. A symbolic description may be obtained induc-
tively, in which case f (x) = x will be only an approximation that might not apply outside of
the data distribution. It may be obtained by inspecting the network or even learned using a
simplified setting and fewer parameters. In this case, making the assumption that the sym-
bolic description is correct will produce an exact answer x to any query of the form f(x).
Reasoning, in many cases too, is required to be exact and not approximate, although there
are cases where approximate or human-like reasoning become more efficient than logical
deduction.
     (b) Commonsense and combinatorial reasoning A key distinction worth making
explicit refers to the difference between commonsense knowledge and expert knowledge.
While the former is approximate and difficult to specify, the latter strives to be as precise as
possible.3 We believe that, once equipped with a solid understanding of the value of hybrid
systems, variable manipulation and reasoning, the debate will be allowed to progress from
the question of symbols versus neurons to the more precise research question:
   How to compute and learn with symbols, inside or outside of a neural network, and
   how to compute and reason efficiently, in a precise or approximate setting?
Foundational work on neural-symbolic integration and neurosymbolic AI models and sys-
tems (d’Avila Garcez et al. 2002, 2009; d’Avila Garcez and Lamb 2003; Hammer and Hit-
zler 2007), discussed next, will be relevant as we embark in the journey to seek to answer
the above research question.


3 Forms of neurosymbolic integration

In d’Avila Garcez et al. (2009), correspondences are proven between various logical/sym-
bolic systems and neural networks. The current limits of neural networks are also evalu-
ated. In a nutshell, neural networks are capable of representing propositional logic, non-
monotonic logic programming, propositional modal logic and fragments of first-order
logic, but not full first-order or higher-order logic. This limitation has prompted the recent
work in areas that combine (symbolic) combinatorial reasoning and deep learning (Prates
et al. 2019; Cameron et al. 2020), graph neural networks and neurosymbolic AI, including


3
   Both types of knowledge can exhibit imprecision and may require the modelling of uncertainty, e.g.,
through the use of probability theory. However, commonsense knowledge is implicit and the conclusions
derived from it may need to be retracted frequently in the face of new evidence, whereas expert knowledge
is expected to produce over time conclusions with high confidence.


13
Neurosymbolic AI: the 3rd wave﻿	                                                          12391


logical neural networks, logic tensor networks and extensions thereof to cater for probabil-
istic languages and neural probabilistic logic programming (Serafini and d’Avila Garcez
2016; Stehr et al. 2022; Lamb et al. 2020; Prates et al. 2019; Riegel et al. 2020; Manhaeve
et al. 2018). Logic Tensor Networks (LTN) (Serafini and d’Avila Garcez 2016) use the lan-
guage of full first-order logic with deep learning, translating logical statements into the loss
function rather than into the network architecture as done before. First-order logic state-
ments are mapped onto differentiable real-valued constraints using a many-valued logic
interpretation in the interval [0,1]. The trained network and the logic become communi-
cating modules of a hybrid system, instead of the logic computation being implemented
by the network. This distinction between constructing neural and symbolic modules that
communicate in various ways and constructing translation algorithms from one representa-
tion to another, therefore in a more integrated way, will be explored in what follows and is
currently at the centre of the debate on neurosymbolic AI.
    Among the recent neurosymbolic systems, one can identify quite a variety in range from
integrative to hybrid systems: (Mao et al. 2019) can be seen as a loosely-coupled hybridi-
zation where image classification is combined with reasoning from text data (; Manhaeve
et al. 2018); offers integration by allowing a node in the probabilistic inference tree of
symbolic ML system ProbLog (De Raedt et al. 2007) to be replaced by a neural network;
(Serafini and d’Avila Garcez 2016) takes another step towards integration by using a differ-
entiable many-valued logic in the loss function of a neural network; (Minervini et al. 2020;
Rocktäschel and Riedel 2017) proposes to perform differentiable unification and theorem
proving inside the neural network. This variety might have prompted Henry Kautz to pro-
pose a taxonony for neurosymbolic AI as part of a keynote speech at AAAI 2020 (Kautz
2022), which we discuss further in Sect. 6.3. Out of the neurosymbolic methods and sys-
tems now available, some more integrated others more loosely-coupled, a common ques-
tion emerges: what are the fundamental building blocks, the necessary and sufficient
ingredients of neurosymbolic AI? For example, is the use of an attention layer necessary
or can it be replaced by richer structure such as graph networks (Lamb et al. 2020; Hochre-
iter 2022)? Is the explicit use of probability theory necessary, and in this case inside the
network or at the symbolic level, or both? Is there a real computational gain in combinato-
rial problem solving or theorem proving using neural networks or is this task better left
to the devices of a symbolic system (Cameron et al. 2020; Lamb et al. 2020; Serafini and
d’Avila Garcez 2016)? One thing is now clearer: there is great practical value in the use of
gradient-based learning on distributed representations (LeCun et al. 2015).
    To start answering the above questions, in this paper, we seek to bring attention to the
relevance of adopting a distributed or a localist representation. In a localist representa-
tion (Page 2000), the relevant concepts have an associated identifier. This is typically a
discrete representation. By contrast, in a distributed representation, concepts are denoted
by vectors with continuous values. This raises the issue of which representation, localist
or distributed, might be adequate in a situation or more appropriate in general. Symbolic
ML, epitomized by decision trees and the learning of logic programs (Lloyd 2003), takes a
localist approach while neural networks are distributed, although neural networks can also
be localist and learn logic programs (d’Avila Garcez et al. 2009).
    With either a localist or a distributed representation, within neurosymbolic AI one may
identify systems that translate and encode symbolic knowledge into the set of weights of
a network (d’Avila Garcez and Zaverucha 1999; d’Avila Garcez et al. 2002, 2009; Ham-
mer and Hitzler 2007; Hu et al. 2016) and systems that translate and encode symbolic
knowledge into the network’s loss function (Serafini and d’Avila Garcez 2016; Marra et al.
2019; Xu et al. 2018). The neural-symbolic cycle (d’Avila Garcez et al. 2002) translating

                                                                                    13
12392                                                                     A. d. Garcez, L. C. Lamb


symbolic knowledge into neural networks and vice-versa offers a compiler for neural net-
works, whereby prior knowledge is translated into the network, and a decompiler when-
ever symbolic descriptions are extracted from a trained network. The compiler can either
set-up the network’s initial weights akin to a one-shot learning algorithm which is guided
by knowledge, or define a knowledge-based penalty or constraint which is added to the
network’s loss function.
   A third form of neural-symbolic integration has been proposed in Bengio et al. (2020)
which is based on changing the representation of the networks into factor graphs. Although
change of representation is a worthwhile endeavor on its own right, helping one under-
stand the strengths and limitations of different models and architectures, this third form
of integration creates an intermediate representation with factor graphs in between neural
networks and logical representations, when it is known that direct translation between net-
works and logic is possible. As a result, the use of factor graphs as an intermediate repre-
sentation is yet to be shown useful, perhaps at facilitating the computational complexity of
the translations. Section 6 contains more information about the neurosymbolic AI systems
in each of the above categories as part of a note which seeks to clarify the terminology
being used in AI, ML and DL.


4 Distributed and localist representation

In order to achieve integration between learning and reasoning, neurosymbolic comput-
ing requires a bridge between localist and distributed representations (d’Avila Garcez et al.
2002; Hammer and Hitzler 2007). The success of deep learning indicates that distributed
representations with gradient-based methods are more adequate than localist ones for
learning and optimization. At the same time, the difficulties of neural networks at extrapo-
lation, explainability and goal-directed reasoning point to the need for a bridge between
distributed and localist representations. In neurosymbolic computing, the aim is to repre-
sent symbolically the knowledge learned by a neural network. Reasoning takes place either
symbolically or within the network in distributed form. Despite their differences, both the
symbolic and connectionist paradigms share common characteristics. Change of represen-
tation offers a way of making sense of the value of different neural models and architec-
tures with respect to what is a more formal and better understood area of research: sym-
bolic logic (d’Avila Garcez and Lamb 2003; d’Avila Garcez et al. 2009).
    Neural network-based learning and inference under uncertainty have been expected to
address the brittleness and computational complexity of symbolic systems. Symbolism has
been expected to provide additional knowledge in the form of constraints for learning (Gori
2018), which ameliorate neural network’s well-known catastrophic forgetting or difficulty
with extrapolation in unbounded domains or with out-of-distribution data. The integration
of neural models with logic-based symbolism is expected therefore to provide an AI sys-
tem capable of transfer learning and a bridge between lower-level information processing
(for efficient perception and pattern recognition) and higher-level abstract knowledge (for
reasoning, explainability, extrapolation and planning).
    Suppose that a complex neural network learns a function f(x). Once this function is
known, or more precisely a simplified description of f(x) is known, computationally it
makes sense to use such a representation, not least for the purpose of extrapolation, as
exemplified earlier with the f (x) = x function. One could argue that at this point the neu-
ral network has become superfluous. Symbol manipulation (once symbols have been


13
Neurosymbolic AI: the 3rd wave﻿	                                                       12393


discovered) is key to further learning at new levels of abstraction. This is exemplified well
in Marcus (2020) with the use of the concept of a container which may be learned from
images.
    Among the most promising recent approaches to neural-symbolic integration, so-called
embedding techniques seek to transform symbolic representations into vector spaces where
reasoning can take place through matrix computations over distance functions (Serafini
and d’Avila Garcez 2016; Evans and Grefenstette 2018; Yang et al. 2017). In such systems,
learning of an embedding is carried out using backpropagation. Most of the research in
this area is focused on the art of representing relational knowledge such as P(X, Y) in a
distributed neural network. The logical predicate P relating variables X and Y could be
used to denote, for example, the container relation between two objects in an image such
as a violin and its case, which in turn are described by their embedding. This process is
known as relational embedding. For representing more complex logical structures such as
first order-logic formulas, e.g., ∀X, Y, Z ∶ (P(X, Y) → Q(Y, Z)), a system named Logic Ten-
sor Networks (LTN) (Serafini and d’Avila Garcez 2016) was proposed by extending Neu-
ral Tensor Networks (NTN) (Socher et al. 2013), a state-of-the-art relational embedding
method. Related ideas are discussed in the context of constraint-based learning and reason-
ing in Gori (2018) and Marra et al. (2020).
    Two powerful concepts of LTN are: (1) the grounding of logical concepts onto ten-
sors with the use of logical statements which act as constraints on the vector space to help
learning of an adequate embedding, and (2) the modular and differentiable organisation of
knowledge within the neural network which allows querying and interaction with the sys-
tem. Any user-defined statement in first-order logic can be queried in LTN which checks
if that knowledge is satisfied by the trained neural network. With such a tool, a user can
decide when to keep using a distributed connectionist representation or switch to a localist
symbolic representation. This last aspect brings to the fore the question of the emergence
of symbols and their meaning. Recent work using the weak supervision of auto-encoders
and ideas borrowed from disentanglement have been showing promise w.r.t. learning of
relevant concepts which can be re-used symbolically (Chen and Batmanghelich 2019).
Related work seeking to explore the advantages of distributed representations of logic
include (Yang et al. 2017), which is based on stochastic logic programs, (Evans and Gre-
fenstette 2018), with a focus on inductive programming, and Minervini et al. (2020), based
on differentiable programming and theorem proving. Program synthesis based more gener-
ally on probabilistic languages other than logic have also flourished as part of the research
area known as neurosymbolic programming (Chaudhuri et al. 2021).



5 Foundations of neurosymbolic computing

In symbolic ML, symbols are manipulated as part of a discrete search for the
best representation to solve a classification or regression task. The most well-
known form of symbolic ML are decision trees, but richer forms of representa-
tion exist, in particular relational representations using first-order logic to denote
concepts ranging over variables X, Y, Z... within a (possibly infinite) domain, e.g.,
∀X, Y, Z ∶ grandfather(X, Y) ← (father(X, Z) ∧ mother(Z, Y)) (the father of someone’s
mother is that person’s grandfather). Probabilistic extensions of this approach seek to
learn probability distributions for such logical rules (or functional programs) as a way
of accounting for uncertainty in the training data. Work in these areas is probably best

                                                                                  13
12394                                                                 A. d. Garcez, L. C. Lamb


characterised by research in Statistical Relational Learning (SRL) (Raedt et al. 2016;
Richardson and Domingos 2006).
   All of the excitement and industrial interest in the past 10 years surrounding AI
and Machine Learning, though, have come from an entirely separate type of ML: deep
learning. Deep learning uses neural networks and stochastic gradient descent to search
through a continuous space, also to solve a given classification or regression task, but
creating vector-based, distributed representations, rather than logical or symbolic rep-
resentations. For this reason, such systems are called sub-symbolic. While it is clear
now that AI will not be achieved by building expert systems by hand from scratch, but
by learning from large collections of data, one would be misguided to conflate all of
machine learning or dismiss the role of symbolic logic, which remains the most power-
ful and adequate representation for the analysis of computational systems (Fagin et al.
2003). As put simply by Moshe Vardi “Logic is the Calculus of Computer Science” and,
differently from statistics, machine learning can only exist within the context of a com-
putational system.
   Specifically, deep neural networks will require a language for description, as also
advocated by Valiant. Neural network-based AI is distributed and continuous, deals
well with large-scale multimodal noisy perceptual data such as text and audio, handles
symbol grounding better than symbolic systems since concepts are grounded on feature
vectors, and is by definition a computational model, frequently implemented efficiently
using propagation of activation and tensor processing units. Symbolic AI is generally
localist and discrete, capable of sophisticated reasoning, including temporal, epistemic
and nonmonotonic reasoning, planning, extrapolation and reasoning by analogy. Neu-
rosymbolic AI has shown that non-classical logics, in particular many-valued logics,
offer an adequate language for describing neural networks (d’Avila Garcez et al. 2009;
Serafini and d’Avila Garcez 2016; Riegel et al. 2020). As the field of AI moves towards
agreement on the need for combining the strengths of neural and symbolic AI, it can
turn next to the question: What is the best representation for Neurosymbolic AI?
   To answer this question, one should seek to be informed by developments in neuro-
symbolic computing of the past 20 years, and to evaluate in a precise manner the meth-
ods, algorithms and applications of neurosymbolic AI. For instance, it is known that
current recurrent neural networks are capable of computing the logical consequences
of propositional modal logic programs and other forms of non-classical reasoning and
fragment of first-order logic programs (Bader et al. 2007; d’Avila Garcez et al. 2009).
Obtaining results for full first-order logic has not been possible thus far, which rein-
forces John McCarthy’s claim that neural networks are in essence systems for proposi-
tional logic (McCarthy 1988).
   Recent efforts at question answering and language translation as well as protein fold-
ing classification have highlighted the importance of hybrid AI systems (d’Avila Garcez
et al. 2019; Lamb et al. 2020; Jumper et al. 2020). The ideal type of application for a
neurosymbolic system, however, should be that where abstract information is required
to be reasoned about at a level of abstraction that goes beyond that what can be per-
ceived from data alone, such as complex concept learning whereby simpler concepts are
required to be organised systematically before the definition of a higher-level concept
can be learned. Such a conceptual structure requires knowledge which is governed by
general rules and exceptions to the rules, allowing for sound generalization in the face
of uncertainty but also capable of handling specific cases (the many exceptions, which
are important for the sake of robustness but not statistically relevant).


13
Neurosymbolic AI: the 3rd wave﻿	                                                          12395


6 A note about terminology

In Pearl (2019), Turing award winner Judea Pearl offers a critique of machine learning
which, unfortunately, conflates the terms machine learning and deep learning. Similarly,
when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that
of expert systems dispossessed of any ability to learn. The use of the terminology is in
need of clarification. Machine learning is not confined to association rule mining, c.f. the
body of work on symbolic ML and relational learning (Raedt et al. 2016). The main differ-
ences between symbolic ML and deep learning are (i) the choice of representation - local-
ist logical in symbolic ML rather than distributed in the case of DL, and (ii) the non-use
of gradient-based learning algorithms by symbolic ML. Equally, symbolic AI is not just
about production rules written by hand. A proper definition of AI concerns knowledge rep-
resentation and reasoning, autonomous multi-agent systems, planning and argumentation,
as well as machine learning. In what follows, we elaborate on the above misunderstandings
one at a turn.

6.1 Symbolic ML vs. deep learning

In Pearl (2019), Pearl proposes a hierarchy consisting of three levels: association, interven-
tion and counterfactual reasoning, and claims that ML is only capable of achieving associa-
tion. A purely symbolic or neurosymbolic ML system should in fact be capable of satisfy-
ing the requirements of all three of Pearl’s levels, e.g., by mapping the neural networks
onto symbolic descriptions. It is fair to say in relation to Pearl’s top level in the hierarchy—
counterfactual reasoning—that progress has only been made recently within ML and that
much research is still needed, although good progress is being made towards the extraction
of local, measurable counterfactual explanations from black box ML systems (White and
d’Avila Garcez 2019). Once a neural network has been endowed with a symbolic interpre-
tation, one has no reason to doubt the ability of a neural system of asking what-if ques-
tions. In fact, the very algorithm for extracting symbolic logic descriptions of the form
A → B from trained neural networks (d’Avila Garcez et al. 2002) uses a form of interroga-
tion of the network akin to the intervention of Bayesian models advocated by Pearl. We
argue therefore that a more important question is representational: which representation
is most effective, deep networks or Bayesian networks? While attempting to answer this
question, as well as considering the demands of the practical applications, it is important to
recognise that neural networks offer a concrete model of computation trained by differenti-
able learning algorithms, one which can be implemented efficiently by message passing or
propagation of activation, differently from Bayesian networks. A limitation of having such
a concrete computational model may be in the difficulty of pure neural networks at model-
ling rich forms of abstraction which are not dependent on the data (images, audio, etc.) but
which exist instead at a higher conceptual level.

6.2 Knowledge representation and reasoning in AI

Complex problem solving using AI requires a much richer language than that of expert sys-
tems as suggested by Hinton. AI requires a language that can go well beyond Horn clauses
to include relational knowledge, time and other modalities, negation by failure, variable
substitution and quantification, etc. The use of first-order logic (FOL) may not require
instantiating (or grounding) all possible combinations of the values of the variables (e.g.,

                                                                                    13
12396                                                                      A. d. Garcez, L. C. Lamb


X and Y in a relation R(X, Y)), even though most SRL approaches based on FOL do require
grounding, making inference time consuming, e.g., Markov Logic Networks (Richardson
and Domingos 2006). In relational reasoning with neural networks, borrowing from the
field of relational databases, it is typically the grounded (and therefore propositional rather
than first-order) representation that is learned and reasoned about. To avoid confusion, we
would term this latter task relationship learning. Two other equally important attributes
of a rich language for complex problem solving are compositionality, in the sense of the
compositionality of the semantics of a logical language, and modularity. It is worth not-
ing that in the original paper about deep learning (Hinton et al. 2006), before much of the
attention turned to Convolutional Neural Networks, modularity was a main objective of
the proposed semi-supervised greedy learning of stacks of restricted Boltzmann machines
(RBMs). The recently-proposed stacked capsule autoencoders (Kosiorek et al. 2019) and
neurosymbolic approaches such as Logic Tensor Networks (Serafini and d’Avila Garcez
2016) as well as other semi-supervised or weakly-supervised approaches revive the impor-
tant stance of modularity in neural computation. Earlier efforts towards modularity in neu-
rosymbolic AI can be traced back to the system for Connectionist Modal and Intuitionistic
Logics (d’Avila Garcez et al. 2009). Modal logics with a possible-world semantics have
been shown to offer a natural approach to modularity in neural computation when applied
to well-known reasoning tasks and puzzles such as the muddy children puzzle (d’Avila
Garcez et al. 2009).
    We therefore take the view that AI encompasses ML and more. We argue for the combi-
nation of statistical ML, knowledge representation (KR), and logical reasoning. By logical
reasoning, we mean not only classical logic reasoning with the traditional true-false inter-
pretation, but non-classical reasoning including nonmonotonic, modal and many-valued
logics (d’Avila Garcez et al. 2009).
    More researchers than ever on both sides of the connectionist-symbolic AI divide are
now open to studying and learning about each others’ tools and techniques. This was not
the case until recently. The use of different terminology alongside some preconceived opin-
ion or perhaps idleness, fueled by the way that science normally rewards research carried
out in silos, have prevented earlier progress. Synergies will lead to faster progress in AI. It
is reassuring to see it happening in this way: the neural information processing community
have shown the value of neural computation in practice, which has attracted the curiosity
of great minds from symbolic AI. We hope that further collaboration in neurosymbolic AI
will help solve many of the outstanding challenges.

6.3 A taxonomy for neurosymbolic AI

Understanding the role of distributed and localist approaches allow us to present an analy-
sis of Henry Kautz’s taxonomy for neurosymbolic AI (Kautz 2022). In Kautz’s taxonomy,
a Type 1 neural-symbolic integration is standard deep learning, which is included to note
that the input and output of a neural network can be made of symbols, e.g., text in the case
of language translation. Type 2 are hybrid systems such as DeepMind’s AlphaGo and other
systems where the core neural network is loosely-coupled with a symbolic problem solver
such as Monte Carlo tree search. Type 3 are systems whereby a neural network focusing on
one task (e.g., object detection) interacts via its input and output with a symbolic system
specialising in a complementary task (e.g., query answering). Examples include the neuro-
symbolic concept learner (Mao et al. 2019) and deepProbLog (Manhaeve et al. 2018). In
a Type 4 neurosymbolic system, symbolic knowledge is compiled into the training set of a


13
Neurosymbolic AI: the 3rd wave﻿	                                                         12397


neural network e.g., Arabshahi et al. (2019); Lample and Charton (2020); Type 4 includes
tightly-coupled, localist systems where forms of symbolic knowledge are translated into
the initial architecture and set of weights of a neural network, in some cases with guaran-
tees of correctness (d’Avila Garcez et al. 2009), e.g., Logical Neural Networks which cre-
ate a 1-to-1 correspondence between neurons and the elements of logical formulas (Riegel
et al. 2020). Type 5 are tightly-coupled, but distributed neural-symbolic systems where a
symbolic logic rule is mapped onto an embedding which acts as a soft-constraint (a regu-
larizer) on the network’s loss function. These include LTN (Serafini and d’Avila Garcez
2016; Stehr et al. 2022) and Tensor Product Representations (Huang et al. 2017), referred
to in d’Avila Garcez et al. (2019) as tensorization methods. Finally, Type 6 systems should
be capable of true symbolic reasoning inside a neural engine, being considered fully-inte-
grated systems. Early work in neurosymbolic AI has achieved this, see (d’Avila Garcez
et al. 2009). Some Type 4 systems are also capable of it, but using localist representations
and simpler forms of embedding than Type 5. Kautz adds that Type 6 systems should be
capable of combinatorial reasoning, possibly by using an attention schema to achieve it.
Recent Type 6 efforts include (Cameron et al. 2020; Lamb et al. 2020; Prates et al. 2019),
although fully-fledged Type 6 systems for combinatorial reasoning do not exist yet.
    Research into Type 5 systems will likely focus on the provision of rich embeddings and
the study of the extent to which such embeddings may correspond either to pre-defined
prior knowledge or to learned attention mechanisms. Further research on Type 6 systems
will be highly relevant to the theory of neurosymbolic computing. In practical terms, a ten-
sion exists between effective learning and sound reasoning, which may prescribe the use
of a more hybrid approach of Type 3 to 5, or variations thereof such as the use of attention
with tensorization. Orthogonal to the above taxonomy, but mostly associated thus far with
Type 4, is the study of the limits of reasoning within neural networks. Recently, this has
been the focus of analyses of DL in symbolic domains (Tavares et al. 2020), which in our
view should include the study of first-order and non-classical logics.


6.4 A note on explainable AI (XAI)

Knowledge extraction is an integral part of neurosymbolic AI. One main difficulty in XAI
is the efficient extraction of compact but correct and complete knowledge. It can be argued
that a large knowledge-base is not more explainable than a large neural network. Although
this may be true at the level of the entire model explainability, in the case of local explana-
tions, i.e., explanations of individual cases, a knowledge-base is certainly more explainable
than a neural network, since it offers a trace (a proof history) showing why an outcome was
obtained, as opposed to simply showing how propagation of activation through the network
has led to that outcome. It is a main goal of knowledge extraction algorithms to seek to
derive compact relevant representations from complex networks.
    Early efforts at knowledge extraction in neurosymbolic AI as a form of explanation were
always evaluated w.r.t. fidelity: a measure of the accuracy of the extracted knowledge in
relation to the neural network rather than the data. High fidelity is therefore fundamental
whenever XAI is to be claimed to offer a good explanation for a complex network. Unfor-
tunately, many recent XAI methods have abandoned fidelity as a measure of the quality of
an explanation, making it easier for an apparently excellent explanation to be wrong, in that
it may simply not be a valid explanation of the given ML model. Even better than fidel-
ity, if an XAI method can be shown to be sound (d’Avila Garcez et al. 2001) then it will
provably converge to high-fidelity. Knowledge extraction should also allow communication

                                                                                   13
12398                                                                     A. d. Garcez, L. C. Lamb


between users and the ML system. It should offer a way of identifying and correcting for
bias in the system, which is a serious and present problem (Kleinberg et al. 2020). As a
result of the General Data Protection Regulation, many companies have decided as a pre-
caution to remove protected variables such as gender from their ML system. It is well
known, however, that proxies exist in the data which will continue to bias the outcome so
that the removal of such variables may serve only to hide bias that otherwise could have
been revealed via knowledge extraction (d’Avila Garcez et al. 2001).
   In summary, at least two options exist for neurosymbolic AI. In Option 1, symbols are
translated into a neural network and one seeks to perform reasoning within the network. In
Option 2, a more hybrid approach is taken whereby the network interacts with a symbolic
system for reasoning. A third option, which would not require a neurosymbolic approach,
exists when expert knowledge is made available, rather than learned from data, and one is
interested in achieving precise sound reasoning as opposed to approximate reasoning. We
discuss each option briefly next.
   In Option 1, it is desirable still to produce a symbolic description of the network for the
sake of improving explainability (discussed further in Sect. 6.4) or trust, or for the purpose
of communication and interaction with the system. In Option 2, by definition, a neuros-
ymbolic interface is needed. This may be the best option currently, in practical terms, to
combine reasoning and learning in AI given the apparent different nature of both tasks (the
discrete and exact nature of logical reasoning and the continuous and approximate nature
of statistical learning). However, the value of distributed approximate reasoning using neu-
ral networks is only starting to be explored as in the case of differentiable neural comput-
ers and neural theorem proving (Minervini et al. 2020). In Option 3, a reasonable require-
ment nowadays would be to compare results with deep learning and the other options. This
is warranted by the latest practical results of deep learning showing that neural networks
can offer, at least from a computational perspective, better results than purely symbolic
systems.
   Finally, the choice between Options 1 and 2 above may depend on the application at
hand and the availability of quality data and knowledge. Scientists will continue to seek to
make sense of the strengths and limitations of both neural and symbolic approaches. On
this front, the research advances faster on the symbolic side due to the clear hierarchy of
semantics and language expressiveness and rigour that exists at the foundation of the area.
By contrast, little is known about the expressiveness of the latest DL models in relation to
established neural models beyond data-driven comparative empirical evaluations. As advo-
cated by Paul Smolensky, neurosymbolic computing can help map the latest neural models
into existing symbolic hierarchies, thus helping organise the extensively ad-hoc body of
work in neural computation (Huang et al. 2017).



7 Challenges for the principled combination of reasoning and learning

For a combined perspective on reasoning and learning, it is useful to note that reasoning
systems may have difficulties computationally when reasoning with existential quantifiers
and function symbols, such as ∃xP(f (x)). Efficient logic-based programming languages
such as Prolog, for example, assume that every logical statement is universally quantified.
By contrast, learning systems may have difficulty when adopting universal quantification
over variables. To be able to learn a universally quantified statement such as ∀xP(x), a
learning systems needs in theory to be exposed to all possible instances of x. This simple

13
Neurosymbolic AI: the 3rd wave﻿	                                                        12399


duality points to a possible complementary nature of the strengths of learning and reason-
ing systems. To learn efficiently ∀xP(x), a learning system needs to jump to conclusions,
extrapolating ∀xP(x) given an adequate amount of evidence (the number of examples or
instances of x). Such conclusions may obviously need to be revised over time in the pres-
ence of new evidence, as in the case of nonmonotonic logics. In this case, a statement
of the form ∀xP(x) becomes a data-dependent generalization, which is not to be assumed
equivalent to a statement ∀yP(y), as done in classical logic. Such statements may have been
learned from different samples of the overall potentially infinite population. On the other
hand, a statement of the form ∃xP(x) is trivial to learn from data by identifying at least one
case P(a), although reasoning from ∃xP(x) is more involved, requiring the adoption of an
arbitrary constant b such that P(b) holds.
    It is now accepted that learning takes place in a continuous search space of (sub)dif-
ferentiable functions; reasoning takes place in general in a discrete space as in the case of
goal-directed theorem proving. The most immediate way of benefiting from the combina-
tion of reasoning and learning, therefore, is to adopt a hybrid approach whereby a neural
network takes care of the continuous search space (and learning of probabilities), while
a symbolic system consisting of logical descriptions of the network uses discrete search
to achieve extrapolation and goal-directed reasoning. As hinted already in this paper, a
property of early deep learning may hold the key to the above hybrid perspective: modu-
larity. In the original paper on deep learning (Hinton et al. 2006), a modular system is
proposed consisting of a stack of restricted Boltzmann machines. As a result of modular-
ity, the extraction of symbolic descriptions from each RBM is made considerably easier
(Tran and d’Avila Garcez 2018). Each RBM learns a joint probability distribution while
their symbolic descriptions reflect the result of learning without manipulating probabilities
explicitly, thus avoiding the complexity of probabilistic inference found in symbolic AI.
    We therefore do not advocate the adoption of monoblock networks with millions of
parameters. Even though this may be how the human brain works, loss of modularity seems
to be, at least at present and from a computational perspective, a price that is too high to
pay. Modularity remains a fundamentally relevant property of any computing system. In
this space, we notice the emergence of neurosymbolic systems where large networks are
combined with a “logical layer" under the assumption that the network (potentially with
millions of parameters) carries out some relevant processing at a lower level of abstraction,
e.g., object detection, before it is worth incorporating any logical concept at a higher-level
of abstraction. An example would be the use of large Convolutional Neural Networks to
which a neurosymbolic network is added only to the fully-connected layers at the end, as
illustrated in Giunchiglia et al. (2022).
    One major way of driving advances in AI continues to be through challenging appli-
cations, be it language translation, computer games or protein folding competitions. Lan-
guage understanding in the broadest sense of the term, including question-answering that
requires commonsense reasoning, offers probably the most complete application area for
neurosymbolic AI. As an example, consider this question and its commonsense answer
from the COPA data set (Roemmele et al. 2011): It got dark outside. What happened as a
result? (a) Snowflakes began to fall from the sky; (b) The moon became visible in the sky.
Another very relevant application domain is planning, which requires learning and rea-
soning over time, as in this example adapted from Schlag and Schmidhuber (2018): Dan-
iel picks up the milk; Daniel goes to the bedroom; Daniel places the milk on the table;
Daniel goes to the bathroom. Where is the milk? Finally, an area where machine learning
and knowledge representation and reasoning have complementary strengths is knowledge
engineering, including knowledge-base completion and data-driven ontology learning. In

                                                                                  13
12400                                                                             A. d. Garcez, L. C. Lamb


this application area, rich and large-scale symbolic representations already exist alongside
data, including knowledge graphs which can be combined with neural networks such as
graph neural networks (Lamb et al. 2020; Carvalho et al. 2022). In the case of language
models, it is difficult to see how the trained network with its learned embeddings could be
discarded. The embeddings provide a context for concepts alongside their properties, that
is, a grounding into the real-world for the symbols. This grounding is not normally present
in purely symbolic form, a notable exception being Pierre Levy’s Information Economy
Meta-language (IEML) (Lévy 2013). IEML is designed bottom-up to account for seman-
tics within the syntax of the language. It should be studied in the context of neurosymbolic
AI because it makes it possible to calculate distances between concepts in symbolic form.
When the neural network is expected to live side-by-side with the symbolic representation,
the symbolic representation may be useful for reasoning about the network’s answers that
goes beyond the reasoning capabilities of the network. In this case the main objective is to
link-up the symbols emerging at different parts of the network with their grounding. Estab-
lishing this connection, normally via the use of knowledge graphs, has been a main area of
ongoing research as described in Carvalho et al. (2022).
    Our conclusion from the above discussion is that in neurosymbolic AI:

(i) Knowledge should be grounded onto vector representations for efficient learning from
data based on message passing in neural networks as an efficient computational model.
    (ii) Symbols should become available as a result of querying and knowledge extraction
from trained networks, and offer a rich description language at a proper level of abstrac-
tion, enabling infinite uses of finite means, but also compositional reasoning at the sym-
bolic level allowing for extrapolation beyond the data distribution.4
    (iii) The combination of learning and reasoning should offer an alternative to the prob-
lem of combinatorial reasoning by learning to reduce the number of effective combina-
tions, thus producing simpler symbolic descriptions as part of the neurosymbolic cycle,
that is, the translation of neural networks into symbolic representations and vice-versa.

As another example, consider a Variational Autoencoder which learns in unsupervised
fashion to maximise mutual information between pixel inputs and a latent code or some
other embedding consisting of fewer relevant features than the number of pixels. Suppose
that this neural network has learned to find regularities such as e.g., when it sees features
of type A, it also sees features of type B but not features of type C. At this point, such
regularities can be converted into symbols: ∀xA(x) → (∃yB(y) ∧ ¬∃zC(z)). As a result of
the use of variables x, y, z at the symbolic level, one can extrapolate the above regular-
ity to any feature of type A, B or C. A symbolic description is also a constraint on the
neurosymbolic cycle. It is generalised from data during learning and it certainly includes
an ability to ask what-if questions (c.f. Sect. 6). Reasoning about what has been learned
allows for extrapolation beyond the data distribution, and finally the symbolic description
can serve as prior knowledge (as a constraint) for further learning in the presence of more
data, which includes the case of knowledge-based transfer learning. Further training can
now take place at the perceptual or sub-symbolic level, or at the conceptual or symbolic



4
   Although knowledge extraction from large networks may not be complete, it should be sound and meas-
urable (d’Avila Garcez et al. 2001; White and d’Avila Garcez 2019). One of the most interesting areas of
recent progress addresses the emergence of symbols and their meaning from trained neural networks (Ngan
et al. 2022).


13
Neurosymbolic AI: the 3rd wave﻿	                                                                12401


level. This is when having a distributed (sub-symbolic) and a localist (symbolic or sub-
symbolic) representation becomes relevant. Assuming that probabilities are dealt with at
the sub-symbolic distributed level (as in RBMs) and that the symbolic level is used for a
more qualitative representation of uncertainty in the form of general rules with exceptions,
we avoid the complications of having to deal simultaneously with discrete and continuous
learning of rules and probabilities.
   A common challenge that will persist, however, is embodied in the question: how sym-
bolic meaning emerges from large networks of neurons?
   Perhaps an important choice for neurosymbolic AI is the choice between combinato-
rial (exact) reasoning and commonsense, approximate reasoning. While learning is always
approximate, reasoning can be approximate or exact. In a neurosymbolic system, it is pos-
sible to envisage the combination of efficient approximate reasoning (jumping to conclu-
sions) with more deliberative and precise or normative symbolic reasoning (d’Avila Garcez
et al. 2015; Valiant 2003). Conclusions may be revised through learning from new obser-
vations and via communication with the system through knowledge extraction and exact
reasoning. One might expect commonsense to emerge as a result of this process of reason-
ing and learning, although the modelling and computing with commonsense knowledge
continues to be a challenge.
   From a practical perspective, a recipe for neurosymbolic AI might be: learning is car-
ried out from data by neural networks which use gradient-descent optimization; efficient
forms of propositional reasoning can also be carried out by the network, c.f. neural-sym-
bolic cognitive reasoning (d’Avila Garcez et al. 2009); rich first-order logic reasoning and
extrapolation needs to be done symbolically from descriptions extracted from the trained
network; once symbolic meaning has emerged from the trained network, symbols can be
manipulated easily and serve as constraints for further learning from data; this establishes a
feasible neurosymbolic AI cycle for learning and reasoning.
   We therefore set out three immediate challenges for neurosymbolic AI, each capable of
spinning out multiple research strands which in our view may become area defining in the
next decade:
   Challenge 1: First-order logic and higher-order knowledge extraction from very large
networks that is provably sound and yet efficient, explains the entire model and local net-
work interactions and accounts for different levels of abstraction.
   Challenge 2: Goal-directed commonsense and efficient combinatorial reasoning about
what has been learned by a complex deep network trained on large amounts of multimodal
data.
   Challenge 3: Human-network communication as part of a multi-agent system that pro-
motes communication and argumentation protocols between the user and an agent that can
ask questions and check her understanding.5




5
  The recent developments in Large Language Models, including OpenAI’s ChatGPT, among others is
related to our point. Humans in the loop, conversing and symbolically interacting with AI systems is an
avenue that received attention recently under the umbrella of reinforcement learning with human feed-
back (RLHF) (Zhu et al. 2023). One can classify RLHF under the taxonomy of Neurosymbolic systems, as
described by Kautz (2022).


                                                                                          13
12402                                                                    A. d. Garcez, L. C. Lamb


8 Summary and future directions

Ingredients of Neurosymbolic AI Narrow AI based on neural networks is already suc-
cessful in practice with big data. There is obvious value in this as shown by the flour-
ishing of the ML community. Here, however, we have been discussing the science of
what constitutes the fundamental ingredients of an intelligent system (Valiant 2003).
One such ingredient, current results show, is gradient-based optimization used by DL to
handle large amounts of data, but other ingredients are needed.
    At the AAAI-2020 fireside conversation with Yoshua Bengio, a question was asked
about the beauty and value of abstract compact symbolic representations such as F = ma
or e = mc2 . Bengio’s answer was to point out that these must have come out of some-
one’s brain (Isaac Newton and Albert Einstein). As Stephen Muggleton noted at another
debate, his goal is to shorten the wait for the next Newton or Einstein. Muggleton’s bet
is on the use of higher-order logic representations in the place of deep learning. With
this example we seek to illustrate that among highly respected researchers, the choice
of AI ingredients may vary widely, from the need for much more realistic models of
the brain to the need for ever more sophisticated forms of abstract computation. With
the neurosymbolic methodology as a third way, the goal is to develop neural network
models with a symbolic interpretation, to learn representations neurally and make them
available for use symbolically (as for example when an AI system is asked to explain
itself). In this paper, we have argued for modularity as an important ingredient, allow-
ing one to refer to large parts of the network by the composition of symbols and rela-
tions among those symbols. Having an adequate language for describing knowledge
encoded in such networks is another important ingredient. We have argued for the use
of first-order logic as this language, as a canonical form of representation, but also other
forms of non-classical logic. Once a complex network can be described symbolically,
ideally by an abstract and compact form as in e = mc2 , any style of deductive reasoning
becomes possible. Reasoning is obviously another fundamental ingredient, either within
or outside the network, exact or approximate.
    In what concerns System 1 (S1) and System 2 (S2) (Kahneman 2011), Kahneman
was clear when he stated that he prefers to use implicit versus explicit thinking and
reasoning. He argued that S1 (the intuitive parallel system) is capable of understand-
ing language, in contradiction with Bengio’s account of S1 and S2 for deep learning.
Kahneman also stated that S2 (as the sequential deliberative system) is most probably
performing symbol manipulation as argued by Marcus (2020). Marcus’s attempt to
define what makes a system hybrid is important inasmuch as a main problem of DL is
a lack of definition. By contrast, LeCun’s recent attempt at defining DL, also presented
at AAAI 2020, falls short of a formal definition. LeCun’s definition fails to distin-
guish deep and shallow networks. Nevertheless, all attempts to create bridges between
S1 and S2 are at this point useful given our state of lack of understanding of how the
brain works. Attempts to create differentiable reasoning are useful, although we would
require an important distinction be made, whether the purpose is to achieve brain-like
systems or to create robust AI. It is possible that these two goals may soon lead into
two quite separate research directions: those who seek to understand and model the
brain and those who seek to achieve or improve AI. Maybe from this perspective the
field is too broad and will require further specialization. Finally, symbolic meaning can
serve to improve performance of S1. In other words, symbols which have been learned,
derived or even invented can act as constraints on the large network and help improve


13
Neurosymbolic AI: the 3rd wave﻿	                                                                  12403


learning performance as part of a positive cycle of learning and reasoning. Constraint
satisfaction as part of the interplay between learning and reasoning is therefore another
ingredient.
    With the above five ingredients of neurosymbolic AI having been specified—gradient-
based optimization, modularity, symbolic language, reasoning and constraint satisfac-
tion—the reader will not be surprised to know that there are many outstanding challenges
for neurosymbolic AI. First, no agreement exists on the best way of achieving the above
combination of language and structure, of knowledge acquired by agents acting in an envi-
ronment and the corresponding reasoning that an agent must implement to achieve its
goals. It is highly desirable, though, that the study of how to achieve the combination of
(symbolic) language and (neural) structure be principled, in that both language and struc-
ture should be specified formally with proofs about their correspondence or lack thereof.
As done in the case of Noam Chomsky’s language hierarchy (Chomsky 1956), proofs are
needed of the capability of different neural architectures at representing various logical lan-
guages. Proofs of correspondence have been shown between neural networks and propo-
sitional, nonmonotonic, modal, epistemic and temporal logic (d’Avila Garcez et al. 2009).
Similar proofs are required for first-order and higher-order logic. The choice of an appro-
priate (logical or probabilistic) language for neurosymbolic AI will play a central role in
the technical debate. In this process, we argue: translate back and forth between representa-
tions, take a principled approach, adopt a language as a constraint on the structure, seek to
provide explanations, combine learning and reasoning, apply, revise and repeat.
    Finally, neurosymbolic AI will need standard benchmarks and associated comprehen-
sibility tests which could, in a principled way, offer a fair comparative evaluation with
other approaches. With benchmarks in neurosymbolic AI, we are less interested in rank-
ings and leaderboards (however useful these are), and we are interested more in the defini-
tion of suitable metrics and representative examples for neurosymbolic AI. Metrics that go
beyond accuracy to address computational complexity, energy consumption and account-
ability in AI, and representative examples, relatively simple and clearly defined tasks, as
in the case of the international SAT competition (http://​www.​satco​mpeti​tion.​org/), capable
of addressing specific learning, knowledge representation and reasoning challenges. The
kind of benchmark for the 3rd wave of AI should address issues related to: learning from
fewer examples, accuracy w.r.t. computing power, performance in the case of extrapola-
tion, transfer learning and concept learning, representative reasoning outside of the training
distribution, e.g., reasoning with variables and reasoning about time, knowledge, uncer-
tainty, etc. The design of such benchmarks has already started, see e.g., d’Avila Garcez
and Jiménez-Ruiz (2022). Just as the field of AI progressed when challenging applications
were set such as chess playing, robot football, self-driving vehicles and protein folding,
neurosymbolic AI should benefit from similar challenges being set by the AI community
specifically for it in the next decade.
Acknowledgements We would like to thank Gary Marcus, Francesca Rossi, Pascal Hitzler, Marco Gori,
Kristian Kersting, Pierre Lévy, Luc de Raedt, Alessandra Russo, Kristian Kersting, Murray Shanahan, Alex-
ander Gray, Moshe Vardi and the anonymous reviewers who helped us improve this manuscript. Luis Lamb
is supported in part by CNPq and CAPES - Finance Code 001.



References
Arabshahi F, Lu Z, Singh S, Anandkumar A (2019) Memory augmented recursive neural networks. https://​
    arxiv.​org/​abs/​1911.​01545


                                                                                            13
12404                                                                               A. d. Garcez, L. C. Lamb


Bader S, Hitzler P, Hölldobler S, Witzel A (2007) The core method: Connectionist model generation for
     first-order logic programs. In: Hammer B, Hitzler P (eds) Perspectives of neural-symbolic integra-
     tion. Springer, Berlin, pp 205–232. https://​doi.​org/​10.​1007/​978-3-​540-​73954-8_9
Bengio Y, Deleu T, Rahaman N, Ke NR, Lachapelle S, Bilaniuk O, Goyal A, Pal CJ (2020) A meta-
     transfer objective for learning to disentangle causal mechanisms. In: ICLR
Cameron C, Chen R, Hartford J, Leyton-Brown K (2020) Predicting propositional satisfiability via end-
     to-end learning. In: AAAI
Carvalho BW, Garcez AD, Lamb LC (2022) Graph-based neural modules to inspect attention-based
     architectures: a position paper. In AAAI fall symposium 2022. Arlington, Virginia. https://​doi.​org/​
     10.​48550/​ARXIV.​2210.​07117. https://​arxiv.​org/​abs/​2210.​07117
Chaudhuri S, Ellis K, Polozov O, Singh R, Solar-Lezama A, Yue Y (2021) Neurosymbolic program-
     ming. Found Trends Program Lang 7(3):158–243
Chen J, Batmanghelich K (2019) Weakly supervised disentanglement by pairwise similarities. https://​
     arxiv.​org/​abs/​1906.​01044
Chomsky N (1956) Three models for the description of language. IRE Trans Inf Theory 2(3):113–124.
     https://​doi.​org/​10.​1109/​TIT.​1956.​10568​13
d’Avila Garcez AS, Besold T, de Raedt L, Földiák P, Hitzler P, Icard T, Kühnberger K, Lamb LC, Miik-
     kulainen R, Silver D (2015) Neural-symbolic learning and reasoning: Contributions and challenges.
     In: AAAI Spring Symposia. http://​www.​aaai.​org/​ocs/​index.​php/​SSS/​SSS15/​paper/​view/​10281
d’Avila Garcez AS, Jiménez-Ruiz E (eds) (2022) Proceedings of the 16th international workshop on neu-
     ral-symbolic learning and reasoning. Cumberland Lodge, Windsor Great Park, Sept 28–30. CEUR
     workshop proceedings, vol 3212. CEUR-WS.org, Aachen. http://​ceur-​ws.​org/​Vol-​3212
d’Avila Garcez AS, Lamb LC (2003) Reasoning about time and knowledge in neural symbolic learning
     systems. In: NIPS, pp 921–928
d’Avila Garcez AS, Zaverucha G (1999) The connectionist inductive learning and logic programming
     system. Appl Intell 11(1):59–77. https://​doi.​org/​10.​1023/A:​10083​28630​915
d’Avila Garcez AS, Broda K, Gabbay DM (2001) Symbolic knowledge extraction from trained neu-
     ral networks: a sound approach. Artif Intell 125(1–2):155–207. https://​doi.​org/​10.​1016/​S0004-​
     3702(00)​00077-1
d’Avila Garcez A, Broda K, Gabbay DM (2002) Neural-symbolic learning systems: foundations and
     applications. Springer, Berlin
d’Avila Garcez AS, Lamb LC, Gabbay DM (2009) Neural-symbolic cognitive reasoning. Springer,
     Berlin-Heidelberg
d’Avila Garcez AS, Gori M, Lamb LC, Serafini L, Spranger M, Tran S (2019) Neural-symbolic comput-
     ing: an effective methodology for principled integration of machine learning and reasoning. FLAP
     6(4):611–632
De Raedt L, Kimmig A, Toivonen H (2007) Problog: a probabilistic prolog and its application in link
     discovery. In: Proceedings of the 20th international joint conference on artifical intelligence.
     IJCAI’07, pp 2468–2473. Morgan Kaufmann Publishers Inc., San Francisco. http://​dl.​acm.​org/​citat​
     ion.​cfm?​id=​16252​75.​16256​73
Evans R, Grefenstette E (2018) Learning explanatory rules from noisy data. JAIR 61:1–64
Fagin R, Halpern JY, Moses Y, Vardi MY (2003) Reasoning about knowledge. MIT Press, Cambridge
Giunchiglia E, Stoian MC, Lukasiewicz T (2022) Deep learning with logical constraints. In: IJCAI-
     ECAI 2022. IJCAI/AAAI Press, Vienna
Gori M (2018) Machine learning: a constraint-based approach. Morgan Kaufmann, Burlington
Hammer B, Hitzler P (eds) (2007) Perspectives of neural-symbolic integration. Springer, Berlin
Hinton GE, Osindero S, Teh Y (2006) A fast learning algorithm for deep belief nets. Neural Comput
     18(7):1527–1554. https://​doi.​org/​10.​1162/​neco.​2006.​18.7.​1527
Hochreiter S (2022) Toward a broad AI. Commun ACM 65(4):56–57
Huang Q, Smolensky P, He X, Deng L, Wu D (2017) A neural-symbolic approach to natural language
     tasks. https://​arxiv.​org/​abs/​1710.​11475
Hu Z, Ma X, Liu Z, Hovy E, Xing E (2016) Harnessing deep neural networks with logic rules. In: ACL
Jumper J, Evans R, et al (2020) High accuracy protein structure prediction using deep learning. In: 14th
     critical assessment of techniques for protein structure prediction, CASP-14
Kahneman D (2011) Thinking. Fast and slow. Farrar, Straus and Giroux, New York
Kautz HA (2022) The third AI summer: AAAI Robert S. Engelmore memorial lecture. AI Mag
     43(1):105–125. https://​doi.​org/​10.​1002/​aaai.​12036
Kleinberg JM, Ludwig J, Mullainathan S, Sunstein CR (2020) Algorithms as discrimination detectors.
     Proc Natl Acad Sci USA 117(48):30096–30100. https://​doi.​org/​10.​1073/​pnas.​19127​90117




13
Neurosymbolic AI: the 3rd wave﻿	                                                                       12405


Kosiorek AR, Sabour S, Teh YW, Hinton GE (2019) Stacked capsule autoencoders. In: NeurIPS 2019, pp
     15486–15496. http://​papers.​nips.​cc/​paper/​9684-​stack​ed-​capsu​le-​autoe​ncode​rs
Lamb LC, d’Avila Garcez AS, Gori M, Prates M, Avelar P, Vardi MY (2020) Graph, neural networks meet
     neural-symbolic computing: a survey and perspective. IJCAI 2020:4877–4884
Lample G, Charton F (2020) Deep learning for symbolic mathematics. In: ICLR. https://​openr​eview.​net/​
     forum?​id=​S1eZY​eHFDS
LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436–444
Lévy P (2013) The semantic sphere 1: computation, cognition and information economy. ISTE. Wiley,
     Hoboken. https://​books.​google.​co.​uk/​books?​id=​EIhS9​DqwLk​gC
Lloyd JW (2003) Logic for learning - learning comprehensible theories from structured data. Cognitive
     Technologies. Springer, Berlin
Manhaeve R, Dumancic S, Kimmig A, Demeester T, De Raedt L (2018) Deepproblog: neural probabilistic
     logic programming. In: Bengio S, Wallach H, Larochelle H, Grauman K, Cesa-Bianchi N, Garnett R
     (eds) NIPS 31, Montreal, pp 3749–3759
Mao J, Gan C, Kohli P, Tenenbaum J, Wu J (2019) The neuro-symbolic concept learner: interpreting scenes,
     words, and sentences from natural supervision. In: ICLR
Marcus G (2020) The next decade in AI: four steps towards robust artificial intelligence. https://​arxiv.​org/​
     abs/​1801.​00631
Marra G, Diligenti M, Giannini F, Gori M, Maggini M (2020) Relational neural machines. https://​arxiv.​org/​
     abs/​2002.​02193
Marra G, Giannini F, Diligenti M, Gori M (2019) Lyrics: a general interface layer to integrate logic infer-
     ence and deep learning. In: Joint European conference on machine learning and knowledge discovery
     in databases, Springer, pp 283–298
McCarthy J (1988) Epistemological challenges for connectionism. Behav Brain Sci 11(1):44–44. https://​doi.​
     org/​10.​1017/​S0140​525X0​00526​4X
Minervini P, Bosnjak M, Rocktäschel T, Riedel S, Grefenstette E (2020) Differentiable reasoning on large
     knowledge bases and natural language. In: AAAI, pp 5182–5190
Ngan KH, Garcez AD, Townsend J (2022) Extracting meaningful high-fidelity knowledge from convolu-
     tional neural networks. In: 2022 international joint conference on neural networks (IJCNN), pp 1–17 .
     https://​doi.​org/​10.​1109/​IJCNN​55064.​2022.​98921​94
Page M (2000) Connectionist modelling in psychology: a localist manifesto. Behav Brain Sci 23(4):443–
     467. https://​doi.​org/​10.​1017/​S0140​525X0​00033​56
Pearl J (2019) The seven tools of causal inference, with reflections on machine learning. Commun ACM
     62(3):54–60. https://​doi.​org/​10.​1145/​32410​36
Prates MOR, Avelar PHC, Lemos H, Lamb LC, Vardi MY (2019) Learning to solve NP-complete problems:
     a graph neural network for decision TSP. In: AAAI
Raedt LD, Kersting K, Natarajan S, Poole D (2016) Statistical relational artificial intelligence: logic, prob-
     ability, and computation. Synthesis lectures on artificial intelligence and machine learning. Morgan &
     Claypool, San Rafael
Richardson M, Domingos P (2006) Markov logic networks. Mach Learn 62(1–2):107–136
Riegel R, Gray A, Luus F, Khan N, Makondo N, Akhalwaya I, Qian H, Fagin R, Barahona F, Sharma U,
     Ikbal S, Karanam H, Neelam S, Likhyani A, Srivastava S (2020) Logical neural networks. https://​arxiv.​
     org/​abs/​2006.​13155
Rocktäschel T, Riedel S (2017) End-to-end differentiable proving. https://​arxiv.​org/​abs/​1705.​11040
Roemmele M, Bejan CA, Gordon AS (2011) Choice of plausible alternatives: an evaluation of common-
     sense causal reasoning. In: AAAI spring symposia. Stanford
Schlag I, Schmidhuber J (2018) Learning to reason with third-order tensor products. In: NeurIPS
Serafini L, d’Avila Garcez AS (2016) Logic tensor networks: deep learning and logical reasoning from data
     and knowledge. https://​arxiv.​org/​abs/​1606.​04422
Socher R, Chen D, Manning C, Ng A (2013) Reasoning with neural tensor networks for knowledge base
     completion. In: NIPS, pp 926–934
Stehr M-O, Kim M, Talcott CL (2022) A probabilistic approximate logic for neuro-symbolic learning and
     reasoning. J Log Algebr Methods Program 124:100719. https://​doi.​org/​10.​1016/j.​jlamp.​2021.​100719
Tavares AR, Avelar PHC, Flach JM, Nicolau M, Lamb LC, Vardi MY (2020) Understanding boolean func-
     tion learnability on deep neural networks. https://​arxiv.​org/​abs/​2009.​05908
Tran S, d’Avila Garcez A (2018) Deep logic networks: inserting and extracting knowledge from deep belief
     networks. IEEE TNNLS 29:246–258. https://​doi.​org/​10.​1109/​TNNLS.​2016.​26037​84
Valiant LG (2003) Three problems in computer science. J ACM 50(1):96–99
White A, d’Avila Garcez A (2019) Measurable counterfactual local explanations for any classifier. https://​
     arxiv.​org/​abs/​1908.​03020



                                                                                                 13
12406                                                                                             A. d. Garcez, L. C. Lamb


Xu J, Zhang Z, Friedman T, Liang Y, den Broeck GV (2018) A semantic loss function for deep learn-
     ing with symbolic knowledge. In: Dy JG, Krause A (eds) ICML. Proceedings of machine learning
     research, vol 80, Stockholm
Yang F, Yang Z, Cohen WW (2017) Differentiable learning of logical rules for knowledge base reasoning.
     In: NIPS, pp 2319–2328. http://​papers.​nips.​cc/​paper/​6826-​diffe​renti​able-​learn​ing-​of-​logic​al-​rules-​for-​
     knowl​edge-​base-​reaso​ning.​pdf
Zhu B, Jiao J, Jordan MI (2023) Principled reinforcement learning with human feedback from pairwise or
     K-wise comparisons. https://​doi.​org/​10.​48550/​ARXIV.​2301.​11270. https://​arxiv.​org/​abs/​2301.​11270

Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and
institutional affiliations.

Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under
a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted
manuscript version of this article is solely governed by the terms of such publishing agreement and applicable
law.




13
